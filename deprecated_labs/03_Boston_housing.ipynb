{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing with Linear Regression\n",
    "\n",
    "In this notebook we will work with the classic [Boston Housing dataset](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset). \n",
    "\n",
    "Let's load the data and takea look at the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "\n",
    "The first step of any machine learnign project is to explore the data a little bit. We already have a general idea because of the description given to us, but let's dig in a tiny bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the shape of your X matrix is always a good idea.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 506 samples (housing records) and 13 columns (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 506 \"labels\" or put another way \"thing we are interested in predicting\".  In this case they are median housing prices, that is the price that the house sold for.\n",
    "\n",
    "Let's put our data into a dataframe with pandas and examine the first 5 rows of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=data.feature_names) # little trick to get the column names in correctly\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can glance over the first 5 rows just to get a \"feel\" for the data, do we notice anything interesting? Well we see that the range of the values for each column is quite different, we need to deal with that. So we can add scaling our data to a list of things to do.  Before we run off and start scaling though, let's take a look at some basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas.DataFrame.describe()\n",
    "\n",
    "This function is very useful to get a \"feeling\" for the dataset that you are working with.  Describe will give you thec ommon statistics of your matrix, the mean, std (standard deviation), min, max, and quantiles.  It's nice to scroll through and see if anything interesting pops out.  Looking through the output below, do you notice anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a few observations that I see.  The `B` column has most of it's data centered around 375-400 range, we can tell this because it's 25-max quantiles are all in that range. That means anything out of that range is probably an outlier.  The standard deviation for age is 28, which is nearly 50% of it's mean (60), which means that the `AGE` column probably has a _lot_ of variation in it. Finally just scanning the `count` row, they all have 506, which means we don't have any missing values, but we should double check that.  Note that these kinds of observations may lead nowhere or they may lead somewhere.  Getting to know your data is very important, maybe the most important thing you can do as a data scientist. So definitely take your time and write down your thoughts.\n",
    "\n",
    "Let's check for any empty values in our data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info() # this is a simple function that will get us some basic info about our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling our dataset\n",
    "\n",
    "Ok, we need to scale our dataset.  We learned about two choices so you can pick from two options!\n",
    "\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "\n",
    "The order of operations goes like this:\n",
    "1. initialize a scaler\n",
    "2. call `fit` on our dataset\n",
    "3. transform our dataset and return that value to a new variable.\n",
    "\n",
    "Note: `.fit` and `.transform` are called seperately for some sneaky tricky reasons we will get into a bit later. For now just think to yourself about _why_ sklearn _might_ have seperate `.fit` and `.transform` functions on it's scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scaler here\n",
    "\n",
    "scaler = # your choice of scaler here (check the imports to remember their names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit your scaler here -- fitting happens \"in place\" which means it doesn't return anything\n",
    "\n",
    "scaler.fit(#put in our dataset here#)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform our dataset now and return the value\n",
    "\n",
    "X_transformed = #use the scaler transform function on our dataset to return a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "Ok it's time to train our model.  We will want to choose one of the three options we imported earlier.\n",
    "\n",
    "* LinearRegression\n",
    "* SGDRegressor\n",
    "* Ridge\n",
    "\n",
    "## Scikit-Learn API \n",
    "\n",
    "Many (most?) objects in scikit-learn have a `.fit` method which will tell the model to do whatever it was designed to learn.  From the objects that have a `.fit` method, you will get either a follow up `.transform` or `.predict` method.  Objects with `.transform` are transfromers (like the scaler we just used), their job is to learn some statistics from the data and transform it according to some logic (using the statistics that were learned).  Objects that have a `.predict` method are machine learning algorithms, and when you call `.fit` on them they are going to learn whatever rules / functions that the algorithm is designed to learn. When you call `.predict` with them, they will take as input a data point(s) and give you a prediction(s) for the input.\n",
    "\n",
    "So, in summary we frequently see three methods\n",
    "\n",
    "* `.fit` : this tells the object to learn\n",
    "* `.transform` : this tells the object to apply it's learning in the form a transformation to the input. This method _returns_ an object (the transformed data\n",
    "* `.predict` : this tells the object to make a prediction on the input, using whatever function the model learned (from the `.fit` call it made earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model here\n",
    "\n",
    "reg = # initialize your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here with .fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## umm... are we done?\n",
    "So we fit a model right? Now unlike our previous work, we can't really plot this model, it's not a line in 2d, it's a plane in 13d.  So what can we do?  We need to evaluate our model somehow! What should we do?\n",
    "How about we make a bunch of predictions and see what kind of accuracy it gets?  We can evaluate it's mean-squared error, the same metric we used to optimize it.\n",
    "\n",
    "# Evaluate our model\n",
    "\n",
    "Ok, let's use our trained model to make predictions, then we can evaluate those predictions against the real known `y` values. You will need to use your models `.predict()` function which needs some input to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use your model to make predictions on the data\n",
    "\n",
    "y_pred = # your model.predict here  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we need to evaluate our predictions. We will use scikit-learns inbuilt mean squared error metric for this. It's very important that you pass your arguments correctly to the evaluation function, all scikit-learn metrics use `y_true, y_pred` format, which means pass the ground-truth first, followed by the prediction.\n",
    "\n",
    "Well, actually in this case MSE is the same regardless which argument goes first because of the way the math works out, but that isn't always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mean_squared_error(y, y_pred)\n",
    "print (f\"the MSE is:{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a wrap!\n",
    "\n",
    "So.. are we done now? Are you happy with our model?  What does our MSE even mean? Is it good? Is it bad? Would you be ready to roll this model out to production?\n",
    "What questions do you have?\n",
    "\n",
    "## Is this Model Overfit?\n",
    "How do we have any idea if this model is overfit or not? Ideally we'd be able to plot it and visualize it. But we cannot! It is in 13D.\n",
    "Do you have any reason to believe this model would _generalize_ to unseen data?  If yes, what is it? If no, why not?\n",
    "Something _else_ is needed.... what is it?\n",
    "\n",
    "## Please go on the forums and post about this question!\n",
    "\n",
    "Try not to read other peoples response who are ahead of you in the course.  In fact, when you get ahead make sure to help the newbies out and not spoil the surprise for them!\n",
    "What is the funny feeling in our stomach from? What \"smells\" wrong here?\n",
    "\n",
    "[LINK TO FORUM TOPIC](https://forum.codingnomads.co/t/post-your-answer-to-the-question-what-is-funny-in-my-boston-housing-problem-is-my-model-good/344)\n",
    "\n",
    "## Bonus work\n",
    "Try fitting all the 3 different models we created.  Further, see if you can make the MSE lowes by adjusting any of the parameters (regularization?) that we have learned about so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
