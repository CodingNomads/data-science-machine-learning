{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "# classifiers we will use\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model selection bits\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# plotting\n",
    "from plotting import plot_learning_curve, plot_validation_curve\n",
    "\n",
    "#dataset\n",
    "from sklearn.datasets import load_digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Classification\n",
    "\n",
    "In this notebook you will practice making different ensemble classifiers working with a set of handwritten digits. Sklearn gives us a copy of the test set (so it will be smaller than decription you read below).\n",
    "\n",
    "Our aim is to build some models and practice with the parameters. We want to adjust things like\n",
    "* number of estimators\n",
    "* base-estimators parameters\n",
    "\n",
    "Our analysis will be very similar to the last lab, we want to visualize how the parameters are affecting the performance so we can make validation curves. \n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data # data is the flattened version of the images\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a few samples\n",
    "\n",
    "It's always a good idea to \"look\" at our data. Our data is handwritten digits and in fact that means they are stored as 8x8 matrices.  This is stored in `data.images`.  So let's plot a few of those and see what they look like.\n",
    "\n",
    "The astute reader will notice that we have used this code before, in fact we looked at this dataset in the intro notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.images[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data.images[0], cmap='binary', interpolation='nearest');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 10, figsize=(12, 2))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(data.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(data.target[i]),\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Some blurry images.  However, we won't be doing our classification on 8x8 pictures, because all the algorithms we currently know need row vector samples, we will need to `flatten` our images into long vectors that are (64,) in shape (if this sounds insane to you, it is -- but it works surprisingly well, however you would like to learn convolutional neural networks (CNN's) in your future!).  But sklearn is so kind that they have already done this for us, so we just take our data from `data.data` (instead of `data.images`)\n",
    "\n",
    "### Explore the Data\n",
    "\n",
    "Let's investigate the data a bit more. Answer the following questions\n",
    "\n",
    "* Are there any missing data points or features? (probably not, but you should always check!)\n",
    "* How many classes are there?\n",
    "* What does the class distribution look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On your Models, get set, GO!\n",
    "\n",
    "Go ahead and make a your models.  We want AdaboostClassifier and a RandomForestClassifier.\n",
    "Go ahead and read the documentation for both of these models and try out the different parameters.\n",
    "\n",
    "I suggest you try and plot 3 validation curves and 1 learning curve for each model.  How well do they perform? Which one is better? What surprised you?\n",
    "\n",
    "What scoring metric is appropriate for this task? We've been using the f1-score for binary classification, but this is a little different.\n",
    "[Take a read on this thread if you run into any problems](https://forum.codingnomads.co/t/target-is-multiclass-but-average-binary-please-choose-another-average-setting-one-of-none-micro-macro-weighted/811)\n",
    "\n",
    "Also, how many trees will you be creating per graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait ! One more Thing!\n",
    "Before I leave you to it, I need to show you how to set parameters for _base_ estimators. Since adaboost can use any estimator, we may want to try different parameters on it's base estimator.\n",
    "\n",
    "The easy way to do this is as follows:\n",
    "\n",
    "```\n",
    "my_decision_tree = DecisionTreeClassifier(max_depth=10, min_samples_leaf=20)\n",
    "ada = AdaBoostClassifier(my_decision_tree, n_estimators=100)\n",
    "```\n",
    "\n",
    "However, what do we do if we want to run a grid-search or validation curve and change the parameters of the base-estimator?  Then we have a special syntax from sklearn.  It works like this\n",
    "\n",
    "\n",
    "```\n",
    "ada = AdaBoostClassifier(estimator=DecisionTreeClassifier())\n",
    "param_grid = {'estimator__max_depth':[2,10,20],\n",
    "              'estimator__min_samples_leaf':[2,10,20]}\n",
    "grid = GridSearchCV(ada, param_grid, cv=3)\n",
    "```\n",
    "\n",
    "Note that the estimator keyword for Adaboost is `estimator`. So we access the parameters of the `estimator` using the double under `__` and then put the our known estimators.  This kind of \"digging\" to access parameters is a default API in sklearn. In more complicated situations you can go multiple layers, the doucmentation provides nice examples here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/grid_search.html#composite-estimators-and-parameter-spaces\n",
    "\n",
    "\n",
    "Ok, now you know enough to be dangerous, so let's make some models!\n",
    "I suggest you try at least 3 validation curves for adaboost and randonforests.  Since they can both share the same base-estimator you can try and compare how the same settings affect each learning algorithm differently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
